from typing import Tuple, List, Iterator, Dict, Union, cast
from numpy.typing import NDArray
from argparse import Namespace
from torch import Tensor

import random

import torch
import torchaudio
import numpy as np

from .input_encoder import AbstractInputEncoder
from src.data.filters import (
    filter_audio_channel, resample_channel, loud_norm, normalize_wave
)
from src.utils.norm_image import StatsRecorder


class AudioInputEncoder(AbstractInputEncoder):
    """
    Standard audio input encoder, converts to spectograms
    """
    def __init__(self, config: Namespace) -> None:
        super().__init__(config)

        # Processing
        self.samplerate: int = config.samplerate
        self.loudness: int = config.loudness
        self.norm_wave: bool = config.norm
        self.norm_spec: bool = config.norm

        # Padding
        self.max_ms: int = config.max_ms

        # Augmentations
        self.aug: bool = config.aug
        self.aug_pad: bool = config.aug_pad
        self.aug_shift: bool = config.aug_shift
        self.aug_volume: bool = config.aug_volume
        self.aug_spec: bool = config.aug_spec
        self.aug_spec_pct: float = config.aug_spec_pct
        self.aug_spec_time: int = config.aug_spec_time
        self.aug_spec_freq: int = config.aug_spec_freq

        # Mel spectogram
        self.n_fft: int = config.n_fft
        self.n_mels: int = config.n_mels
        self.hop_len: int = config.hop_len

    def fit(self, inputs: Iterator[List[Tuple[int, NDArray]]]) -> None:
        """
        Fit the input encoder to the train data

        Calculates normalization stats if norm options is set. Otherwise, always
        calculates input_dimi

        :param inputs Iterator[List[Tuple[int, NDArray]]]: Generator iterating
        over entire train split, as loaded by the designated Loader
        """
        if self.norm_spec:
            self.spec_stats: StatsRecorder = StatsRecorder()
            self.norm_spec = False
            for input_ in inputs:
                spectogram = self.transform(input_, False)
                self.spec_stats.update(spectogram[None, :])
            # Un-batch since we're not broadcasting across batches
            self.spec_stats.mean = self.spec_stats.mean[0, :]
            self.spec_stats.std = self.spec_stats.std[0, :]
            self.norm_spec = True
        else:
            spectogram = self.transform(next(inputs), False)
        self._input_dim = spectogram.numpy().shape  # type: ignore

    def transform(self, input_: List[Tuple[int, NDArray]], is_train: bool) -> Tensor:
        """
        Filters, normalizes and augments the raw audio, before converting to a
        spectogram the model can consume

        :param input_ List[Tuple[int, NDArray]]: Single datapoint from a Loader
        :param is_train bool: Whether the datapoint is in the train set or not
        :rtype Tensor: Transformed data
        """
        channels = self.process_channels(input_, is_train)
        channels = self.pad_trunc_channels(channels, is_train)
        if self.aug and is_train and self.aug_shift:
            channels = self.aug_channels(channels)

        # (channels, n_mels, time)
        spectogram = self.to_spectogram(channels)
        if self.aug and is_train and self.aug_spec:
            spectogram = self.aug_spectogram(spectogram)

        return spectogram

    def collate_fn(
        self, batch: List[Dict[str, Union[Tensor, int]]]
    ) -> Dict[str, Tensor]:
        """
        Collate function to use for batching audio data

        :param batch List[Dict[str, Union[Tensor, int]]]: List of datapoints as
        generated by Dataset.__get_item__(). Datapoints are dictionaries with
        "input" and "target" keys, each mapping to a datapoint generated by
        Input Encoder and Target Encoder transform() methods
        :rtype Dict[str, Tensor]: Dictionary with "input" and "target" keys,
        corresponding to input and target tensors to feed into model
        """
        # The cast() is to make Mypy type checking happy
        batch_input = torch.stack([cast(Tensor, d["input"]) for d in batch])
        batch_target = torch.tensor([d["target"] for d in batch])

        return {
            "input": batch_input,
            "target": batch_target,
        }

    def process_channels(
        self, sr_and_channels: List[Tuple[int, NDArray]], is_train: bool
    ) -> List[NDArray]:
        """
        Filters, resamples and normalizes a each input channel

        :param sr_and_channels List[Tuple[int, NDArray]]: Single datapoint, list
        of channels where each channel contains sample rate and data as numpy
        array
        :param is_train bool: Whether the datapoint is in the train set or not
        :rtype List[NDArray]: List of processed channels
        """
        sample_rates, channels = list(zip(*sr_and_channels))
        filtered = [
            filter_audio_channel(sr, d)
            for sr, d in zip(sample_rates, channels)
        ]
        processed = [
            resample_channel(d, sr, self.samplerate)
            if sr != self.samplerate else d
            for sr, d in zip(sample_rates, filtered)
        ]

        if self.norm_wave:
            processed = [
                normalize_wave(d)
                for d in channels
            ]
        elif self.loudness:
            processed = [
                loud_norm(sr, d, self.loudness, is_train, shift=self.aug_volume)
                for sr, d in zip(sample_rates, processed)
            ]

        return processed

    def pad_trunc_channels(
        self, channels: List[NDArray], is_train: bool
    ) -> NDArray:
        """
        Pad or truncates each input channel to a consistent length

        :param channels List[NDArray]: List of input channels
        :param is_train bool: Whether the datapoint is in the train set or not
        :rtype NDArray: Input channels with a standardized length
        """
        max_len = int(self.samplerate * self.max_ms / 1000)

        resized_channels = []
        for channel in channels:
            l = len(channel)
            if l > max_len:
                channel = channel[:max_len]
            elif l < max_len:
                diff = max_len - l
                if self.aug and is_train and self.aug_pad:
                    start_pad_len = random.randint(0, diff)
                else:
                    start_pad_len = 0
                end_pad_len = diff - start_pad_len

                start_pad = np.zeros(start_pad_len)
                end_pad = np.zeros(end_pad_len)
                channel = np.concatenate((start_pad, channel, end_pad), axis=0)
            resized_channels.append(channel)
        return np.array(resized_channels)

    def aug_channels(self, channels: NDArray, shift_pct: float = 0.1) -> NDArray:
        """
        Augments each input channel by shifting left or right

        :param channels NDArray: Input channels with a standardized length
        :param shift_pct float: Max percent of length to shift by
        :rtype NDArray: Shifted input channels
        """
        # channels must be a numpy array
        for channel in channels:
            shift_amt = int(random.random() * shift_pct * len(channels))
            channel[:] = np.roll(channel, shift_amt)
        return channels

    def to_spectogram(self, channels: NDArray, top_db: int = 80) -> Tensor:
        """
        Convert input channels to spectogram

        :param channels NDArray: Processed input channels
        :param top_db int: Top dB for spectogram
        :rtype Tensor: Spectogram
        """
        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc
        transform = torchaudio.transforms.MelSpectrogram(
            self.samplerate,
            n_fft=self.n_fft,
            hop_length=self.hop_len,
            n_mels=self.n_mels
        )
        spectogram = transform(torch.tensor(channels, dtype=torch.float32))

        # Convert to decibels
        transform = torchaudio.transforms.AmplitudeToDB(top_db=top_db)
        spectogram = transform(spectogram)

        if self.norm_spec:
            spectogram = (
                (spectogram - self.spec_stats.mean) / self.spec_stats.std
            )

        return spectogram

    def aug_spectogram(self, spectogram: Tensor) -> Tensor:
        """
        Augment spectogram

        :param spectogram Tensor: Spectogram to augment
        :rtype Tensor: Augmented spectogram
        """
        _, n_mels, n_steps = spectogram.shape
        mask_value = spectogram.mean()
        aug_spec = spectogram

        freq_mask_param = int(self.aug_spec_pct * n_mels)
        for _ in range(self.aug_spec_freq):
            transform = torchaudio.transforms.FrequencyMasking(freq_mask_param)
            aug_spec = transform(aug_spec, mask_value)

        time_mask_param = int(self.aug_spec_pct * n_steps)
        for _ in range(self.aug_spec_time):
            transform = torchaudio.transforms.TimeMasking(time_mask_param)
            aug_spec = transform(aug_spec, mask_value)

        return aug_spec


class RawAudioInputEncoder(AudioInputEncoder):
    """Raw audio input encoder, only resamples and reshapes"""

    def fit(self, inputs: Iterator[List[Tuple[int, NDArray]]]) -> None:
        """
        Fit the input encoder to the train data, only sets _input_dim

        :param inputs Iterator[List[Tuple[int, NDArray]]]: Generator iterating
        over entire train split, as loaded by the designated Loader
        """
        wav = self.transform(next(inputs), False)
        self._input_dim = wav[0].shape

    def transform(self, input_: List[Tuple[int, NDArray]], is_train: bool) -> Tensor:
        """
        Resamples and reshapes the raw audio

        :param input_ List[Tuple[int, NDArray]]: Single datapoint from a Loader
        :param is_train bool: Whether the datapoint is in the train set or not
        :rtype Tensor: Transformed data
        """
        sample_rates, channels = list(zip(*input_))
        resampled = [
            resample_channel(d, sr, self.samplerate)
            if sr != self.samplerate else d
            for sr, d in zip(sample_rates, channels)
        ]
        padded = self.pad_trunc_channels(resampled, is_train)
        return torch.tensor(np.squeeze(np.array(padded))).float()
